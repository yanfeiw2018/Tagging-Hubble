{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "remove_terms = punctuation\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages, including nltk for word tokenization/sentence tokenization, spacy for lemmatization, gensim for phrase detection model,  sklearn for tfidf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df1.csv')\n",
    "df1.head() # has all the necessary columns, but keyword and keyword_norm are not clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_csv('df_key_0620.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for i in range(500):\n",
    "    a = ast.literal_eval(df_check['key_0620'][i])\n",
    "    keywords = keywords + a\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dict_final = Counter(keywords)\n",
    "sorted_x = sorted(dict_final.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1 = pd.read_csv('df_key_v1.csv')\n",
    "#This version only has 3 columns, bibcode and cleaned 'keyword' and 'keyword_norm' (nan was replaced with placeholders). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "f = open('reuters_wos.txt')\n",
    "reu_stop = f.read().split()\n",
    "stop_words.extend(x for x in reu_stop if x not in stop_words)\n",
    "len(stop_words)\n",
    "#Initial stopwords include nltk stopword collection and common stopwords in scientific publications by Thomson Reuters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized stop words for the pre-processing step\n",
    "places = []\n",
    "with open('pre2_stop.txt', 'r') as filehandle:  \n",
    "    for line in filehandle:\n",
    "        currentPlace = line[:-1]\n",
    "        places.append(currentPlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(x for x in places if x not in stop_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are preprocessing functions.\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]    \n",
    "    filtered_text = ' '.join(filtered_tokens) # re-create document from filtered tokens\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus, lemm = True, stopword_removal = True):\n",
    "    normalized_corpus = []\n",
    "    for doc in corpus:\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = doc.lower()\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc) # remove extra newlines\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A) # remove special characters\n",
    "        if lemm:\n",
    "            doc = lemmatize_text(doc)\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc)\n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1.loc[0,'keyword'] # is a string!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all keyword and keyword_norm\n",
    "keyword_list = [];\n",
    "for i in range(len(df_key_v1)):\n",
    "    keyword_list.append(df_key_v1.loc[i,'keyword'])\n",
    "keyword_norm_list = [];\n",
    "for i in range(len(df_key_v1)):\n",
    "    keyword_norm_list.append(df_key_v1.loc[i,'keyword_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all keyword and keyword_norm; however, no need to remove stopwords\n",
    "norm_keyword_all = [];\n",
    "norm_keyword_norm_all = [];\n",
    "for i in range(len(df_key_v1)): \n",
    "    norm_keyword_all.append(normalize_corpus(ast.literal_eval(keyword_list[i]), lemm = True, stopword_removal = False))\n",
    "    norm_keyword_norm_all.append(normalize_corpus(ast.literal_eval(keyword_norm_list[i]), lemm = True, stopword_removal = False))\n",
    "# 'galaxies evolution' should be the same as 'galaxy evolution'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the above cell as well. taking too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_set_1 = [];\n",
    "for i in range(len(norm_keyword_all)):\n",
    "    key_set_1.append(list(set(norm_keyword_all[i] +norm_keyword_norm_all[i])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['key_set_1'] = key_set_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1.columns\n",
    "# key_set_1 is combined cleaned-up keyword and keyword-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1.to_csv('/Users/yanfeiwang/Downloads/df_key_v1.csv', index=False)\n",
    "# version control: ['bibcode', 'keyword', 'keyword_norm', 'key_set_1'] with key_set_1 being combined cleaned-up keyword and keyword-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding how many unique phrases/ keywords are in key_set_1, to use them for training bigram\n",
    "key_norm = []\n",
    "key_norm_norm = []\n",
    "for i in range(len(df_key_v1)):\n",
    "     for _ in norm_keyword_all[i]:key_norm.append(_)\n",
    "for i in range(len(df_key_v1)):\n",
    "     for _ in norm_keyword_norm_all[i]:key_norm_norm.append(_)\n",
    "print(len(key_norm), len(np.unique(key_norm)), len(key_norm_norm), len(np.unique(key_norm_norm)))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to titles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = [];\n",
    "for i in range(len(df1['title'])):\n",
    "    title_list.append(ast.literal_eval(df1.loc[i,'title'])[0])\n",
    "len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing of titles\n",
    "norm_title_all = normalize_corpus(title_list, lemm = True, stopword_removal = True)\n",
    "title2word = [[text for text in doc.split()] for doc in norm_title_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['title2word'] = title2word\n",
    "df_key_v1.columns\n",
    "#'title2word' is cleaned up titles in the format of list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with tfidf model, because that might give us better idea of what kind of high-frequency words there are. \n",
    "Use subsets of data to speed up the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_list = df1['abstract']\n",
    "np.unique(abs_list.isna(), return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms = punctuation #+ '0123456789'#\n",
    "remove_terms\n",
    "# for now, leave the numbers in the texts, because they might be object names, eg, ngc 4343. \n",
    "# But in the end, remove them because numbers alone are not keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function serves to parse paragraph data: for abstracts and full-texts, it's useful to still keep the sentence \n",
    "structure during tokenization, such as in the case of bi-gram training''' \n",
    "def tokenize_para (para):\n",
    "    sent = sent_tokenizer.tokenize(para) # string to sentences, return a list of sentences;\n",
    "    sent_word = []; # break the sentence into words, return a list of words\n",
    "    for i in range(len(sent)):\n",
    "        sent_word.append(word_tokenize(sent[i])); # a list of lists of words\n",
    "    sent_filt1 = [[word for word in sent if word not in remove_terms] for sent in sent_word]\n",
    "    sent_filt1 = [' '.join(tok_sent) for tok_sent in sent_filt1] # a list of full sentences (each sentence is a string)\n",
    "    norm_sent_filt1 = normalize_corpus(sent_filt1) # return the same as above, but after normalization\n",
    "    norm_00 = [tok_sent for tok_sent in norm_sent_filt1 if len(tok_sent.split()) > 3]\n",
    "    texts = [[text for text in doc.split()] for doc in norm_sent_filt1]\n",
    "    para_styles = {'sent': norm_sent_filt1,   # a list of full sentences (each sentence is a string)\n",
    "                  'word': texts}  # a list of list of words\n",
    "    return para_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 abstracts, for tfidf model, for the purpose of getting more stop_words\n",
    "abs_sents = [];\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        abs_sents.append(tokenize_para(abs_list[i])['sent'])       \n",
    "    except:\n",
    "        abs_sents.append(['nan'])  \n",
    "len(abs_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the 1000 abstracts\n",
    "abs_all_sent1 = [];\n",
    "abs_n_sent = [];\n",
    "for i in range(len(abs_sents)):\n",
    "    number_of_sentences = 0\n",
    "    for j in range(len(abs_sents[i])):\n",
    "        number_of_sentences = number_of_sentences + 1;\n",
    "        abs_all_sent1.append(abs_sents[i][j])\n",
    "    abs_n_sent.append(number_of_sentences)\n",
    "print(len(abs_all_sent1), len(abs_n_sent))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodylist = df1['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out introduction\n",
    "intro_list = [];\n",
    "for i in range(len(bodylist)):\n",
    "    try:\n",
    "        try1 = bodylist[i].lower()\n",
    "    except:\n",
    "        try1 = ['nan']\n",
    "    try:\n",
    "        split1 = try1.split('introduction')\n",
    "        try:\n",
    "            split2 = split1[1].split('observations', 1)\n",
    "        except:\n",
    "            intro = ['nan']\n",
    "    except:\n",
    "        intro = ['nan']\n",
    "    try:\n",
    "        intro = split2[0]\n",
    "    except:\n",
    "        intro = ['nan']\n",
    "    intro_list.append(intro)\n",
    "len(intro_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out conclusion\n",
    "conc = [];\n",
    "for i in range(len(bodylist)):\n",
    "    try:\n",
    "        try1 = bodylist[i].lower()\n",
    "    except:\n",
    "        try1 = ['nan']\n",
    "    try:\n",
    "        split1 = try1.split('conclusions')\n",
    "        if 'thank' in split[1]:\n",
    "            split2 = split1[1].split('thank')\n",
    "        elif 'acknowledge' in split[1]:\n",
    "            split2 = split1[1].split('acknowledge')\n",
    "        else:\n",
    "            try:\n",
    "                split2 = split1[1].split('references')\n",
    "            except:\n",
    "                split2 = ['nan']\n",
    "    except:\n",
    "        intro = ['nan']\n",
    "    try:\n",
    "        intro = split2[0]\n",
    "    except:\n",
    "        intro = ['nan']\n",
    "    conc.append(intro)\n",
    "len(conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_conc = [];\n",
    "for i in range(len(bodylist)):\n",
    "    intro_conc.append(intro_list[i]+conc[i])\n",
    "len(intro_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_body_all = normalize_corpus(intro_conc, lemm = True, stopword_removal = True)\n",
    "#body2word = [[text for text in doc.split()] for doc in norm_body_all]\n",
    "len(norm_body_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(norm_body_all[16407])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(norm_body_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['norm_body'] = norm_body_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1.columns\n",
    "# 'norm_body_all' is cleaned-up introduction+ conclusion from maintext, as one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_body_5 = [];\n",
    "for i in range(5):\n",
    "    norm_body_5.append(norm_body_all[i])\n",
    "norm_body_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF model to get important words\n",
    "tvec3 = TfidfVectorizer(min_df=0.005, max_df=0.2)\n",
    "\n",
    "X3 = tvec3.fit_transform(norm_body_5)\n",
    "abs_dict_list = [];\n",
    "weights3 = [];\n",
    "for i in range(X3.shape[0]):\n",
    "    \n",
    "    weights3_0 = np.squeeze(X3[i].toarray())\n",
    "    df3 = pd.DataFrame({'term': tvec3.get_feature_names(), 'frequency': weights3_0})\n",
    "    tfidf_dict = df3.sort_values(by='frequency', ascending=False).head(40).to_dict()\n",
    "    abs_dict_list.append(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4plot = pd.DataFrame.from_dict(abs_dict_list[0])\n",
    "df4p1 = df4plot.sort_values(by='frequency', ascending=False).head(25)\n",
    "plt.xlabel(\"Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "plt.barh(df4p1.term, df4p1.frequency, align='center', color='#3F5D7D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Iterate through preprocessing and tfidf to construct two vocabularies of stop-words\n",
    "### - Iterate the above for more text (publication 1000-2000, 2000-3000, etc)\n",
    "### - Partition the high-frequency words into the two vocabularies of stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sentences = Text8Corpus(datapath('testcorpus.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences, min_count=1, threshold=1)\n",
    "print(phrases[title2word[0]])\n",
    "# random file 'sentences' do not help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_key_norm = df_key_v1['key_set_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_key_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "key_norm = [];\n",
    "for i in range(len(total_key_norm)):\n",
    "    key_norm.append(ast.literal_eval(total_key_norm[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(key_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_norm_all = []\n",
    "for i in range(len(key_norm)):\n",
    "    key_norm_all.extend(x for x in key_norm[i] if x not in key_norm_all)\n",
    "len(key_norm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word4bigram = [word_tokenize(key) for key in key_norm_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and clean all sentences, to prepare for building bigram model on entire list of maintext\n",
    "## run on google colab\n",
    "body_sents_words = [];\n",
    "for i in range(len(norm_body_all)):\n",
    "    try:\n",
    "        body_sents_words.append(tokenize_para(norm_body_all[i])['word'])       \n",
    "    except:\n",
    "        body_sents_words.append([['nan']]) \n",
    "len(body_sents_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_sents_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all list of list of words for full texts\n",
    "all_full_texts = [];    \n",
    "for i in range(len(body_sents_words)):\n",
    "    all_full_texts = all_full_texts + body_sents_words[i]\n",
    "len(all_full_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_full_texts = [];    \n",
    "for i in range(1000):\n",
    "    all_full_texts = all_full_texts + body_sents_words[i]\n",
    "len(all_full_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and clean all sentences, to prepare for building bigram model on entire list of abstracts\n",
    "## takes too long to run on local computer\n",
    "## run on google colab\n",
    "sents = [];\n",
    "for i in range(len(abs_list)):\n",
    "    try:\n",
    "        sents.append(tokenize_para(abs_list[i])['word'])       \n",
    "    except:\n",
    "        sents.append([['nan']]) \n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all list of list of words for abstracts\n",
    "all_abstracts = [];    \n",
    "for i in range(len(sents)):\n",
    "    all_abstracts = all_abstracts + sents[i]\n",
    "len(all_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['abs_words'] = sents\n",
    "#'abs_words' is tokenized, cleaned abstract , a list of list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [];\n",
    "for i in range(1000,2000):\n",
    "    try:\n",
    "        sents.append(tokenize_para(abs_list[i])['word'])       \n",
    "    except:\n",
    "        sents.append([['nan']]) \n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all list of list of words for abstracts\n",
    "all_abstracts = [];    \n",
    "for i in range(len(sents)):\n",
    "    all_abstracts = all_abstracts + sents[i]\n",
    "len(all_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a bigram model on all titles, keywords, keyword_norms, all abstracts, 1000 full papers\n",
    "bigram2 = gensim.models.phrases.Phrases(title2word+word4bigram+all_abstracts+all_full_texts, min_count=1, threshold=0.005) \n",
    "for i in range(100):\n",
    "    print(bigram2[all_abstracts[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(bigram1[title2word[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(bigram2[title2word[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_key_19 = []\n",
    "for i in range(len(title2word)):\n",
    "    title_key_19.append(bigram1[title2word[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a bigram model on all titles, keywords, keyword_norms, all abstracts, 1000 full papers\n",
    "bigram1 = gensim.models.phrases.Phrases(title2word+word4bigram+all_abstracts+all_full_texts, min_count=1, threshold=1) \n",
    "for i in range(10):\n",
    "    print(bigram1[all_abstracts[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a trigram model\n",
    "trigram1 = Phrases(bigram1[n_gram_vocab], min_count=1, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    trigrams_ = [t for t in trigram1[bigram1[all_abstracts[i]]]if t.count('_')==2]\n",
    "    print(trigrams_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Training with the ensemble of all titles helps: was able to pick up zeta_ophiuchi, galactic_halo\n",
    "- #### Moving forward, using bigram1 model to fit bigrams on everything\n",
    "- #### batch-wise fitting\n",
    "- #### Trigram did not generate more useful keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs_list to extract keywords\n",
    "abs_list=['nan' if x is np.nan else x for x in abs_list]\n",
    "norm_abs_all = normalize_corpus(abs_list, lemm = True, stopword_removal = True)\n",
    "len(norm_abs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['norm_abs'] = norm_abs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram model on entire list of abstracts\n",
    "abs_2gram =[]\n",
    "for i in range(len(sents)):\n",
    "    abs_2gram.append([]);\n",
    "    for j in range(len(sents[i])):\n",
    "        abs_2gram[i].append(bigram1[sents[i][j]])\n",
    "len(abs_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace the space between 2 words that appear as a bigram\n",
    "### in gensim model with _\n",
    "matchingwords = [];\n",
    "for i in range(len(sents)):\n",
    "    for j in range(len(sents[i])):\n",
    "        for k in reversed(range(len(sents[i][j]))):\n",
    "            try: \n",
    "                mystring = sents[i][j][k]+'_'+sents[i][j][k+1]  \n",
    "                if mystring in abs_2gram[i][j]:\n",
    "                    mytuple = (sents[i][j][k], sents[i][j][k+1])\n",
    "                    sents[i][j][k] = '_'.join(mytuple)\n",
    "                    sents[i][j].remove(sents[i][j][k+1])\n",
    "                else:\n",
    "                    sents[i][j][k] = sents[i][j][k]\n",
    "            except:\n",
    "                sents[i][j][k] = sents[i][j][k]\n",
    "            sents[i][j]\n",
    "    matchingwords.append(sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matchingwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(matchingwords[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs_sent_list is list of all abstracts( in the format of sentences as individual strings after bi-gram replacement)\n",
    "abs_list = [];\n",
    "for i in range(len(sents)):\n",
    "    abs_sent_list = [];\n",
    "    for j in range(len(matchingwords[i])):\n",
    "        \n",
    "        text_2gram = ' '.join(matchingwords[i][j])\n",
    "        abs_sent_list.append(text_2gram)\n",
    "    abs_list.append(abs_sent_list)\n",
    "print(len(abs_list))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_para = [];\n",
    "for i in range(len(abs_list)):\n",
    "    abs_para_2 = '';\n",
    "    for j in range(len(abs_list[i])):\n",
    "        abs_para_2 = abs_para_2 + abs_list[i][j]\n",
    "    abs_para.append(abs_para_2)\n",
    "len(abs_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchingwords[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['abs_bi_para'] = abs_para\n",
    "#'abs_bi_para'is abstract as one string after bi-gram replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do tfidf on abs_para\n",
    "tvec3 = TfidfVectorizer(min_df=0.005, max_df=0.2)\n",
    "X19 = tvec3.fit_transform(abs_para)\n",
    "\n",
    "abs_dict_list = [];\n",
    "weights19 = [];\n",
    "for i in range(X19.shape[0]):\n",
    "    \n",
    "    weights19_0 = np.squeeze(X19[i].toarray())\n",
    "    df19 = pd.DataFrame({'term': tvec3.get_feature_names(), 'frequency': weights19_0})\n",
    "    tfidf_dict = df19.sort_values(by='frequency', ascending=False).head(100).to_dict()\n",
    "    abs_dict_list.append(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_dict_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4plot = pd.DataFrame.from_dict(abs_dict_list[1])\n",
    "df4p1 = df4plot.sort_values(by='frequency', ascending=False).head(26)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.xlabel(\"Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "plt.barh(df4p1.term, df4p1.frequency, align='center', color='#3F5D7D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Post-n-gram stopwords removal\n",
    "places = []\n",
    "with open('post2_stop.txt', 'r') as filehandle:  \n",
    "    for line in filehandle:\n",
    "        currentPlace = line[:-1]\n",
    "        places.append(currentPlace)\n",
    "\n",
    "stop_words.extend(x for x in places if x not in stop_words)\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-phrase detection stopword removal and \n",
    "from string import punctuation\n",
    "remove_terms2 = punctuation + '0123456789'\n",
    "\n",
    "abs_list_2 = [];\n",
    "for i in range(len(matchingwords)):\n",
    "    abs_sent_list = [];\n",
    "    for j in range(len(matchingwords[i])):\n",
    "        for k in reversed(range(len(matchingwords[i][j]))):\n",
    "            if (matchingwords[i][j][k] in stop_words) or (len(matchingwords[i][j][k]) < 3):\n",
    "                matchingwords[i][j].remove(matchingwords[i][j][k])\n",
    "                # remove keywords that are one character long\n",
    "            else:\n",
    "                str2list = list(matchingwords[i][j][k])\n",
    "                matches = [x for x in str2list if x in remove_terms2]\n",
    "                if len(matches) == len(str2list):\n",
    "                    matchingwords[i][j].remove(matchingwords[i][j][k])\n",
    "                # break the keyword into single chars -- in order to do count -- , then remove keywords \n",
    "                # that are completely made up with numbers and punctuations\n",
    "        text_2gram = ' '.join(matchingwords[i][j])\n",
    "        abs_sent_list.append(text_2gram)\n",
    "    abs_list_2.append(abs_sent_list)\n",
    "print(len(abs_list_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs_para_2 is a list of all abstracts, each as a string, after phrase extraction and more stopwords removal\n",
    "abs_para_2 = [];\n",
    "for i in range(len(abs_list_2)):\n",
    "    abs_para_ = '';\n",
    "    for j in range(len(abs_list_2[i])):\n",
    "        abs_para_ = abs_para_ + abs_list_2[i][j]\n",
    "    abs_para_2.append(abs_para_)\n",
    "len(abs_para_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do tfidf on abs_para after post-n-gram stopwords removal\n",
    "tvec3 = TfidfVectorizer(min_df=0.0005, max_df=0.2)\n",
    "X3 = tvec3.fit_transform(abs_para_2)\n",
    "\n",
    "abs_dict_list2 = [];\n",
    "for i in range(X3.shape[0]):\n",
    "    weights19_0 = np.squeeze(X3[i].toarray())\n",
    "    df3 = pd.DataFrame({'term': tvec3.get_feature_names(), 'frequency': weights19_0})\n",
    "    tfidf_dict = df3.sort_values(by='frequency', ascending=False).head(100).to_dict()\n",
    "    abs_dict_list2.append(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_dict_list2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4plot = pd.DataFrame.from_dict(abs_dict_list2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4p1 = df4plot.sort_values(by='frequency', ascending=False).head(18)\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.xlabel(\"Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "plt.barh(df4p1.term, df4p1.frequency, align='center', color='#3F5D7D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['abs_tfidf'] = abs_dict_list2\n",
    "#'abs_tfidf' is output of tfidf model on abstracts , after bi-gram replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_4_abs = []\n",
    "for i in range(len(abs_dict_list)):\n",
    "    key_4_abs_ = [v for v in abs_dict_list2[i]['term'].values()]\n",
    "    key_4_abs.append(key_4_abs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(key_4_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_key_old = [ast.literal_eval(x) for x in df_key_v1['key_set_1']]\n",
    "len(title_key_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(len(title_key_old[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title_key_old)):\n",
    "    title_key_old[i].extend(x for x in title_key_19[i] if x not in title_key_old[i])   \n",
    "for i in range(10):\n",
    "    print(len(title_key_old[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(key_4_abs)):\n",
    "    title_key_old[i].extend(x for x in key_4_abs[i] if x not in title_key_old[i])   \n",
    "for i in range(10):\n",
    "    print(len(title_key_old[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(len(title_key_old)):\n",
    "    for j in reversed(range(len(title_key_old[_]))):\n",
    "        try: \n",
    "            title_key_old[_][j] = title_key_old[_][j].replace(\"_\", \" \")\n",
    "        except:\n",
    "            title_key_old[_][j] = title_key_old[_][j]\n",
    "            \n",
    "len(title_key_old)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title_key_old)):\n",
    "    for j in reversed(range(len(title_key_old[i]))):\n",
    "        title_key_old[i]=[x for x in title_key_old[i] if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(len(title_key_old[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_key_old[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_v1['key_0629'] = title_key_old\n",
    "df_key_v1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store title_key_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_0629 = df_key_v1[['bibcode','key_0629']]\n",
    "df_key_0629.to_csv('/Users/yanfeiwang/Downloads/df_key_0629.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
